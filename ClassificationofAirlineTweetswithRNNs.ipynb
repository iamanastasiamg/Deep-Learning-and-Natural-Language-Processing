{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb174e70-73ed-4e63-a7da-0d0d02a63a14",
   "metadata": {},
   "source": [
    "### Exercise 2: Classification of Airline Tweets with RNNâ€™s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d688cfa4-2b77-484b-bdca-58c1a40208ec",
   "metadata": {},
   "source": [
    "##### Import the libraries necessary for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "701fa331-62ca-45f7-b3bc-bacdab81cfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amagklara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amagklara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Bidirectional, Attention, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfe8bc8-18f1-4c61-905d-d1a15b005951",
   "metadata": {},
   "source": [
    "##### Data Preprocessing: Loading the dataset and preprocessing steps as in Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfcded05-372e-4525-b96d-1df4a8fb72ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                   said .\n",
       "1         plus 've added commercial experience ... tacky .\n",
       "2         n't today ... must mean need take another trip !\n",
       "3        's really aggressive blast obnoxious `` entert...\n",
       "4                                  's really big bad thing\n",
       "                               ...                        \n",
       "14635                 thank got different flight chicago .\n",
       "14636    leaving 20 minute late flight. warning communi...\n",
       "14637                        please bring american airline\n",
       "14638    money , change flight , n't answer phone ! sug...\n",
       "14639    8 ppl need 2 know many seat next flight. plz p...\n",
       "Name: text, Length: 14640, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_unwanted(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    emojis = re.compile(\n",
    "        \"[\\U0001F600-\\U0001F64F\" # Emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "        \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "        \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "        \"]\", flags=re.UNICODE\n",
    "    )\n",
    "    text = emojis.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    sentence = remove_unwanted(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    tokens = word_tokenize(sentence, language='english', preserve_line=True)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    text = \" \".join(filtered_tokens)\n",
    "    return filtered_tokens\n",
    "\n",
    "tweets_df = pd.read_csv(\"datasets/Tweets.csv\", encoding=\"utf-8\")\n",
    "tokens = [preprocessing(sentence) for sentence in tweets_df['text']]\n",
    "tweets_df['text'] = [\" \".join(token) for token in tokens]\n",
    "tweets_df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfa3dcb-ed98-4428-87fb-f5abbe275140",
   "metadata": {},
   "source": [
    "##### Encode sentiment labels and set X as texts and numeric labels as y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d475332-a455-40c4-8d73-3c5a16b85216",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "tweets_df['label'] = label_encoder.fit_transform(tweets_df['airline_sentiment'])\n",
    "X = tweets_df['text']\n",
    "y = tweets_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "205a2ab3-2695-4f01-bade-7b5e327c9340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming that 'text' column contains the tweet text and 'sentiment' column has the target labels (e.g., positive/negative/neutral)\n",
    "\n",
    "\n",
    "# Tokenizing and padding the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "X_pad = pad_sequences(X_seq, padding='post')\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bd73e9-b076-41c8-93bd-77d686aaf9d4",
   "metadata": {},
   "source": [
    "##### Model Training: Build RNN with Bi-LSTM, stacking multiple RNN layers and attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f50c00-c34a-4d64-aa02-5ad0596dcd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture: Stacked Bi-LSTM with Attention\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(Embedding(vocab_size, 64, input_length=(X_train.shape[1], X_train.shape[-1])))\n",
    "\n",
    "# Adding Bidirectional LSTM layers\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, input_shape=(None, 1))))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Final output layer: a single neuron with sigmoid for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#history = model.fit(X_train, y_train, batch_size = 10, epochs = 12, verbose = 0, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83221997-92cd-447d-9b30-67703ce2032a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_pad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train_pad\u001b[49m, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test_pad, y_test))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Plot training & validation loss and accuracy\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_pad' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Plot training & validation loss and accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training and validation loss curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot the training and validation accuracy curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea1c58-8005-4483-8544-3a53c03fc6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Make predictions\n",
    "y_pred = (model.predict(X_test_pad) > 0.5).astype('int32')\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907fd4ec-c4e0-42e6-95d8-8aa78d05ac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explanation of the Model Architecture\n",
    "#Model Architecture Description:\n",
    "\n",
    "#Embedding Layer: Converts the input words into dense vector representations.\n",
    "#Bidirectional LSTM: The bidirectional LSTM helps capture both past and future context in the text, improving understanding of word relationships.\n",
    "###Attention Mechanism: The attention layer allows the model to focus on important words in the input sequence, improving performance in understanding long dependencies within the text.\n",
    "##Global Average Pooling: This reduces the sequence to a fixed-size vector that summarizes the input sequence.\n",
    "#Dense Layers: The model finishes with a dense layer for classification, using a sigmoid activation function because it is a binary classification problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
