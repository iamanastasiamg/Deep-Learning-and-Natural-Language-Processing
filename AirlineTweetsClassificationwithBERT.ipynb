{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "070cfe46-fabf-42cc-9903-6b944b10c863",
   "metadata": {},
   "source": [
    "### Exercise 3: Classification of Airline Tweets with Pretrained Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6743b9-f391-4b69-9023-36ce975eaff1",
   "metadata": {},
   "source": [
    "##### Import the libraries necessary for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d839b0-cc17-4b09-8984-c9a64026cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61f854d-0d96-4e4e-999d-b68b46e85e71",
   "metadata": {},
   "source": [
    "##### Data Preprocessing: Loading the dataset and preprocessing steps as in Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2ff06fc-b78d-4739-a536-4fedcc037f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    emojis = re.compile(\n",
    "        \"[\\U0001F600-\\U0001F64F\" # Emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "        \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "        \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "        \"]\", flags=re.UNICODE\n",
    "    )\n",
    "    text = emojis.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    sentence = remove_unwanted(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    tokens = word_tokenize(sentence, language='english', preserve_line=True)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    text = \" \".join(filtered_tokens)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "184378c4-0078-4dc8-a6dc-e17f19470d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file):\n",
    "    df = pd.read_csv(data_file, encoding=\"utf-8\")\n",
    "    tokens = [preprocessing(sentence) for sentence in df['text']]\n",
    "    df['text'] = [\" \".join(token) for token in tokens]\n",
    "    texts = df['text'].tolist()\n",
    "    labels = [1 if sentiment == \"positive\" else 0 for sentiment in df['airline_sentiment'].tolist()]\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(df['airline_sentiment'].tolist())\n",
    "    label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "    print(f'The labels of the dataset are: {label_mapping}')\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47f261f2-aec7-4fd6-b2bc-e722a9625a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The labels of the dataset are: {'negative': 0, 'neutral': 1, 'positive': 2}\n"
     ]
    }
   ],
   "source": [
    "texts, labels = load_data(\"datasets/Tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15309359-4f4f-4a89-a465-9ccd79915449",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "def train(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)\n",
    "\n",
    "def predict_sentiment(text, model, tokenizer, device, max_length=128):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "    return \"positive\" if preds.item() == 2 else (\"neutral\" if preds.item() == 1 else \"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff82b3d2-8d5a-428b-8850-c6e03ce0b786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set up parameters\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "num_classes = 3\n",
    "max_length = 128\n",
    "batch_size = 16\n",
    "num_epochs = 4\n",
    "learning_rate = 2e-5\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "bert_model = BERTClassifier(bert_model_name, num_classes)\n",
    "\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# Move your model to the selected device\n",
    "model = bert_model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8d90e9b-2ba6-4b45-a010-0b5b7245b842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Updated loss function with weights\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Initialize a display object for updating output in-place\n",
    "display_id = 'batch_update'\n",
    "display_obj = display(\"\", display_id=display_id)\n",
    "\n",
    "# Initialize lists to store history\n",
    "train_losses = []\n",
    "train_f1_scores = []\n",
    "val_losses = []\n",
    "val_f1_scores = []\n",
    "patience = 0\n",
    "\n",
    "# Initialize variables to track best validation loss and corresponding model weights\n",
    "best_val_loss = float('inf')\n",
    "best_val_f1 = float('-inf')\n",
    "best_model_weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b30163e-bb5f-4760-bcb5-8706e4e3b6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "     train(model, train_dataloader, optimizer, scheduler, device)\n",
    "     accuracy, report = evaluate(model, val_dataloader, device)\n",
    "     print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "     print(report)\n",
    "\n",
    "torch.save(model.state_dict(), \"bert_classifier.pth\")\n",
    "\n",
    "# Test sentiment prediction\n",
    "test_text = \"The movie was great and I really enjoyed the performances of the actors.\"\n",
    "sentiment = predict_sentiment(test_text, model, tokenizer, device)\n",
    "print(\"The movie was great and I really enjoyed the performances of the actors.\")\n",
    "print(f\"Predicted sentiment: {sentiment}\")\n",
    "\n",
    "# Test sentiment prediction\n",
    "test_text = \"Worst movie of the year.\"\n",
    "sentiment = predict_sentiment(test_text, model, tokenizer, device)\n",
    "print(\"Worst movie of the year.\")\n",
    "print(f\"Predicted sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231958fd-3b87-4902-88e1-21aa0ff8963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(train_losses, train_f1_scores, val_losses, val_f1_scores):\n",
    "    \"\"\"\n",
    "    Plot the history of epochs for loss and accuracy\n",
    "    :param train_losses: List of training losses for each epoch\n",
    "    :param train_f1_scores: List of training f1 scores for each epoch\n",
    "    :param val_losses: List of validation losses for each epoch\n",
    "    :param val_f1_scores: List of validation f1 scores for each epoch\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # Plot training and validation losses\n",
    "    plt.plot(epochs, train_losses, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_losses, 'r', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training and validation f1 scores\n",
    "    plt.plot(epochs,train_f1_scores, 'b', label='Training f1-score')\n",
    "    plt.plot(epochs, val_f1_scores, 'r', label='Validation f1-score')\n",
    "    plt.title('Training and Validation =f1-scores')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('f1_score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "history_dict = {\n",
    "    'train_losses': train_losses,\n",
    "    'train_f1_scores': train_f1_scores,\n",
    "    'val_losses': val_losses,\n",
    "    'f1_scores': val_f1_scores,\n",
    "}\n",
    "\n",
    "history_df = pd.DataFrame(history_dict)\n",
    "history_df.to_csv('datasets/history_results.csv', index=False)\n",
    "plot_history(train_losses, train_f1_scores, val_losses, val_f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ebab5f-6c44-4032-bb7f-cdfd984b3086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Confusion Matrix\n",
    "# Predict on test data\n",
    "predictions = trainer.predict(test_data)\n",
    "pred_labels = predictions.predictions.argmax(axis=-1)\n",
    "true_labels = test_data['label']\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
