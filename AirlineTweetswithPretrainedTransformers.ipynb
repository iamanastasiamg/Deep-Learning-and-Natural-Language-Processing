{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7af527cd-5e69-4e86-b7f4-717cdd59f65c",
   "metadata": {},
   "source": [
    "### Exercise 3: Classification of Airline Tweets with Pretrained Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f925d5f-fc09-4045-a83b-ff51224326ea",
   "metadata": {},
   "source": [
    "##### Import the libraries necessary for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d521b93-4c9a-4e13-a83f-e75f6dcb7b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\amagklara\\PycharmProjects\\WordEmbeddings\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer,TFBertModel,BertConfig,TFBertForSequenceClassification,BertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad39f9dc-ddc9-43be-b0db-bbd80f5ebc3f",
   "metadata": {},
   "source": [
    "##### Data Preprocessing: Loading the dataset and preprocessing steps as in Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3995d8fc-9d8a-4f71-8bc3-d7f557fe5589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    emojis = re.compile(\n",
    "        \"[\\U0001F600-\\U0001F64F\" # Emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "        \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "        \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "        \"]\", flags=re.UNICODE\n",
    "    )\n",
    "    text = emojis.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    sentence = remove_unwanted(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    tokens = word_tokenize(sentence, language='english', preserve_line=True)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    text = \" \".join(filtered_tokens)\n",
    "    return filtered_tokens\n",
    "\n",
    "tweets_df = pd.read_csv(\"datasets/Tweets.csv\", encoding=\"utf-8\")\n",
    "tokens = [preprocessing(sentence) for sentence in tweets_df['text']]\n",
    "tweets_df['text'] = [\" \".join(token) for token in tokens]\n",
    "tweets_df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef3a40a-da1f-4796-a7e3-c7ee4b8a36ab",
   "metadata": {},
   "source": [
    "#### 1. Model Fine-Tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26282e1-4ce1-4953-8147-bb29cafe66e2",
   "metadata": {},
   "source": [
    "##### Encode sentiment labels and set X as texts and numeric labels as y. Split the data into train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d17c1b2-1fdc-4536-801d-5fa2de0e3bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "tweets_df['label'] = label_encoder.fit_transform(tweets_df['airline_sentiment'])\n",
    "X = tweets_df['text']\n",
    "y = tweets_df['label']\n",
    "\n",
    "train_input,val_input,train_label,val_label = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba556bd-a59b-4480-acae-15986983db5c",
   "metadata": {},
   "source": [
    "##### Load a Pretrained Transformer Model: ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df02a393-a06a-4299-8b74-6528d074a0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\amagklara\\PycharmProjects\\WordEmbeddings\\.venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe9922cc-38c3-438a-9d08-4f3d089c0ce6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TrainingArguments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 6: Define training arguments\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m(\n\u001b[0;32m      3\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m,          \u001b[38;5;66;03m# output directory for model checkpoints\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,              \u001b[38;5;66;03m# number of training epochs\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,   \u001b[38;5;66;03m# batch size for training\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,    \u001b[38;5;66;03m# batch size for evaluation\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,                \u001b[38;5;66;03m# number of warmup steps for learning rate scheduler\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,               \u001b[38;5;66;03m# strength of weight decay\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m'\u001b[39m,            \u001b[38;5;66;03m# directory for storing logs\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     11\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,     \u001b[38;5;66;03m# Evaluate at the end of each epoch\u001b[39;00m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Step 7: Define Trainer\u001b[39;00m\n\u001b[0;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                         \u001b[38;5;66;03m# the pretrained model\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                  \u001b[38;5;66;03m# training arguments\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m p: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy_score(p\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), p\u001b[38;5;241m.\u001b[39mlabel_ids)}  \u001b[38;5;66;03m# accuracy metric\u001b[39;00m\n\u001b[0;32m     21\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TrainingArguments' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 6: Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory for model checkpoints\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size for training\n",
    "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate at the end of each epoch\n",
    ")\n",
    "\n",
    "# Step 7: Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the pretrained model\n",
    "    args=training_args,                  # training arguments\n",
    "    train_dataset=train_data,            # training dataset\n",
    "    eval_dataset=valid_data,             # validation dataset\n",
    "    compute_metrics=lambda p: {'accuracy': accuracy_score(p.predictions.argmax(axis=-1), p.label_ids)}  # accuracy metric\n",
    ")\n",
    "\n",
    "# Step 8: Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 9: Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6016bc1c-dc52-4cb7-94a1-36de39c60bf3",
   "metadata": {},
   "source": [
    "#### 2. Learning Curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942651e9-7c7f-4cb3-bec0-01237b86ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Plot Learning Curves (Training & Validation Loss)\n",
    "# Extract loss data from trainer\n",
    "train_loss = trainer.state.log_history[::2]  # alternate items are training losses\n",
    "valid_loss = trainer.state.log_history[1::2]  # alternate items are validation losses\n",
    "\n",
    "train_epochs = [x['epoch'] for x in train_loss]\n",
    "train_losses = [x['loss'] for x in train_loss]\n",
    "\n",
    "valid_epochs = [x['epoch'] for x in valid_loss]\n",
    "valid_losses = [x['eval_loss'] for x in valid_loss]\n",
    "\n",
    "# Plot loss curves\n",
    "plt.plot(train_epochs, train_losses, label='Train Loss')\n",
    "plt.plot(valid_epochs, valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curves')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b610795a-edc4-4720-9abc-66f5d4dd956e",
   "metadata": {},
   "source": [
    "#### 3. Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd44594-d42e-4197-89e6-705732828db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Confusion Matrix\n",
    "# Predict on test data\n",
    "predictions = trainer.predict(test_data)\n",
    "pred_labels = predictions.predictions.argmax(axis=-1)\n",
    "true_labels = test_data['label']\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649752c6-1b64-4b8c-8291-f1f3f640bda9",
   "metadata": {},
   "source": [
    "#### 4. Brief Explanation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28202aff-0991-440f-8a51-d566dda28571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
